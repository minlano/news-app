#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í‚¤ì›Œë“œ ì¶”ì¶œê¸°
KoNLPy Oktë¥¼ ì‚¬ìš©í•œ ëª…ì‚¬ ì¶”ì¶œ
"""

import re
import json
from collections import Counter

class KeywordExtractor:
    def __init__(self):
        # KoNLPy ëŒ€ì‹  ê°„ë‹¨í•œ ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì‚¬ìš©
        pass
        
        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì œì™¸í•  ë‹¨ì–´ë“¤)
        self.stopwords = {
            'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ê³³', 'ì ', 'ê°œ', 'ëª…', 'ë²ˆ', 'ì°¨', 'ë…„', 'ì›”', 'ì¼',
            'ì´', 'ê·¸', 'ì €', 'ì˜', 'ë¥¼', 'ì—', 'ê°€', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤',
            'ë˜ë‹¤', 'ê°™ë‹¤', 'ë‹¤ë¥¸', 'ìƒˆë¡œìš´', 'ë§ì€', 'ì¢‹ì€', 'í°', 'ì‘ì€', 'ë†’ì€', 'ë‚®ì€',
            'ê¸°ì', 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ë³´ë„', 'ì·¨ì¬', 'ì¸í„°ë·°', 'ë°œí‘œ', 'ì„¤ëª…', 'ë§',
            'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ì´ë²ˆ', 'ë‹¤ìŒ', 'ì§€ë‚œ', 'ìµœê·¼', 'í˜„ì¬', 'ì•ìœ¼ë¡œ',
            'ì„œìš¸', 'ê²½ê¸°', 'ì¸ì²œ', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…',
            'í•œêµ­', 'ìš°ë¦¬ë‚˜ë¼', 'êµ­ë‚´', 'ì „êµ­', 'ì§€ì—­', 'ì§€ë°©', 'ìˆ˜ë„ê¶Œ', 'ë¹„ìˆ˜ë„ê¶Œ'
        }
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ë‚¨ê¹€)
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def extract_nouns(self, text, min_length=2):
        """ê°„ë‹¨í•œ ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            cleaned_text = self.clean_text(text)
            
            # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
            korean_words = re.findall(r'[ê°€-í£]{2,}', cleaned_text)
            
            # í•„í„°ë§: ê¸¸ì´, ë¶ˆìš©ì–´ ì œê±°
            filtered_words = []
            for word in korean_words:
                if (len(word) >= min_length and 
                    word not in self.stopwords and
                    not word.isdigit()):
                    filtered_words.append(word)
            
            return filtered_words
            
        except Exception as e:
            print(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def get_keyword_frequency(self, texts, top_n=20):
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°"""
        all_nouns = []
        
        for text in texts:
            nouns = self.extract_nouns(text)
            all_nouns.extend(nouns)
        
        # ë¹ˆë„ ê³„ì‚°
        counter = Counter(all_nouns)
        
        # ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ë°˜í™˜
        top_keywords = counter.most_common(top_n)
        
        return top_keywords
    
    def extract_keywords_from_articles(self, articles, top_n=30):
        """ê¸°ì‚¬ë“¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        print(f"ğŸ” ê¸°ì‚¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì œëª© + ìš”ì•½ + ë³¸ë¬¸)
        all_texts = []
        
        for article in articles:
            # ì œëª©
            title = article.get('title', '')
            # ìš”ì•½ (AI ìš”ì•½ì´ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ ì›ë³¸ ìš”ì•½)
            summary = article.get('ai_summary', '') or article.get('summary', '')
            # ë³¸ë¬¸
            content = article.get('content', '')
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            combined_text = f"{title} {summary} {content}"
            all_texts.append(combined_text)
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keywords = self.get_keyword_frequency(all_texts, top_n)
        
        print(f"âœ… ìƒìœ„ {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ì¶”ì¶œëœ í‚¤ì›Œë“œ:")
        for i, (keyword, count) in enumerate(keywords, 1):
            print(f"{i:2d}. {keyword} ({count}íšŒ)")
        
        return keywords
    
    def save_keywords(self, keywords, filename="keywords.json"):
        """í‚¤ì›Œë“œë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            keyword_dict = {
                'keywords': [{'word': word, 'count': count} for word, count in keywords],
                'total_keywords': len(keywords)
            }
            
            filepath = f"data/{filename}"
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(keyword_dict, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ í‚¤ì›Œë“œê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return filepath
            
        except Exception as e:
            print(f"âŒ í‚¤ì›Œë“œ ì €ì¥ ì˜¤ë¥˜: {e}")
            return None

if __name__ == "__main__":
    # ê°œë°œìš© í…ŒìŠ¤íŠ¸ ì½”ë“œ
    pass