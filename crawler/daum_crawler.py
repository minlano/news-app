#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰í•˜ê³  ê¸°ì‚¬ ë‚´ìš©ì„ ìˆ˜ì§‘
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os

class DaumNewsCrawler:
    def __init__(self):
        self.base_url = "https://search.daum.net/search"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def search_news(self, keyword, max_articles=10):
        """í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ì—¬ëŸ¬ í˜ì´ì§€ ì§€ì›)"""
        all_articles = []
        page = 1
        
        while len(all_articles) < max_articles and page <= 5:  # ìµœëŒ€ 5í˜ì´ì§€ê¹Œì§€
            # ì²« ë²ˆì§¸ í˜ì´ì§€ì™€ ë‚˜ë¨¸ì§€ í˜ì´ì§€ì˜ íŒŒë¼ë¯¸í„°ê°€ ë‹¤ë¦„
            if page == 1:
                params = {
                    'w': 'news',
                    'nil_search': 'btn',
                    'DA': 'NTB',
                    'enc': 'utf8',
                    'cluster': 'y',
                    'cluster_page': '1',
                    'q': keyword
                }
            else:
                params = {
                    'w': 'news',
                    'nil_search': 'btn',
                    'DA': 'PGD',
                    'enc': 'utf8',
                    'cluster': 'y',
                    'cluster_page': '1',
                    'p': str(page),
                    'q': keyword
                }
            
            page_articles = self._search_single_page(params, max_articles - len(all_articles))
            
            if not page_articles:
                break
            
            all_articles.extend(page_articles)
            page += 1
            time.sleep(1)  # í˜ì´ì§€ ê°„ ìš”ì²­ ê°„ê²©
        
        return all_articles[:max_articles]  # ìš”ì²­í•œ ê°œìˆ˜ë§Œí¼ë§Œ ë°˜í™˜
    
    def _search_single_page(self, params, max_articles_for_page):
        """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            response = requests.get(self.base_url, params=params, headers=self.headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì•„ì´í…œ ì„ íƒì ì‹œë„
            selectors = [
                'div.c-item-content',
                'div.wrap_cont',
                'li.news',
                'div.item-title',
                'div.news-item',
                'article'
            ]
            
            news_items = []
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    news_items = items[:max_articles_for_page]
                    break
            
            if not news_items:
                # ëª¨ë“  ë§í¬ ì¤‘ì—ì„œ ë‰´ìŠ¤ ê´€ë ¨ ë§í¬ ì°¾ê¸°
                all_links = soup.find_all('a', href=True)
                news_links = [link for link in all_links if 'news' in link.get('href', '').lower()]
                
                for i, link in enumerate(news_links[:max_articles_for_page], 1):
                    title = link.get_text(strip=True)
                    if title and len(title) > 10:
                        article_data = {
                            'title': title,
                            'link': link.get('href'),
                            'summary': "",
                            'source': "ë‹¤ìŒë‰´ìŠ¤",
                            'content': ""
                        }
                        articles.append(article_data)
                
                return articles
            
            # ì •ìƒì ì¸ ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬
            for i, item in enumerate(news_items, 1):
                if len(articles) >= max_articles_for_page:
                    break
                    
                try:
                    # ì œëª© ì„ íƒì ì‹œë„
                    title_elem = None
                    title_selectors = ['a.f_link_b', 'a.tit-g', 'a[class*="tit"]', 'a[class*="link"]']
                    
                    for selector in title_selectors:
                        title_elem = item.select_one(selector)
                        if title_elem and title_elem.get_text(strip=True) and len(title_elem.get_text(strip=True)) > 10:
                            break
                    
                    # ì¼ë°˜ a íƒœê·¸ ì‹œë„
                    if not title_elem:
                        all_links = item.find_all('a')
                        for link in all_links:
                            text = link.get_text(strip=True)
                            if text and len(text) > 15 and 'http' in link.get('href', ''):
                                title_elem = link
                                break
                    
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = title_elem.get('href', '')
                    
                    # ìš”ì•½ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    summary_selectors = ['p.c-item-text', '.summary', '.desc', 'p']
                    summary = ""
                    for selector in summary_selectors:
                        summary_elem = item.select_one(selector)
                        if summary_elem:
                            summary = summary_elem.get_text(strip=True)
                            break
                    
                    # ì–¸ë¡ ì‚¬ ì •ë³´
                    source_selectors = ['span.c-item-source', '.source', '.press']
                    source = "ë‹¤ìŒë‰´ìŠ¤"
                    for selector in source_selectors:
                        source_elem = item.select_one(selector)
                        if source_elem:
                            source = source_elem.get_text(strip=True)
                            break
                    
                    if title and len(title) > 5:
                        article_data = {
                            'title': title,
                            'link': link,
                            'summary': summary,
                            'source': source,
                            'content': ""
                        }
                        
                        articles.append(article_data)
                        time.sleep(0.2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    
                except Exception as e:
                    continue
            
            return articles
            
        except Exception as e:
            return []
    
    def get_article_content(self, article_url):
        """ê°œë³„ ê¸°ì‚¬ì˜ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        try:
            response = requests.get(article_url, headers=self.headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ ë³¸ë¬¸ ì„ íƒì ì‹œë„
            content_selectors = [
                'div.article_view',
                'div#harmonyContainer',
                'div.news_end',
                'div.article-body',
                'div.article_body',
                'div.read_body'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(strip=True)
                    break
            
            return content[:1000] if content else ""  # 1000ìë¡œ ì œí•œ
            
        except Exception as e:
            print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ""
    
    def save_articles(self, articles, filename="articles.json"):
        """ìˆ˜ì§‘í•œ ê¸°ì‚¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        data_dir = "data"
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)
        
        filepath = os.path.join(data_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ê¸°ì‚¬ ë°ì´í„°ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return filepath
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return None

if __name__ == "__main__":
    # ê°œë°œìš© í…ŒìŠ¤íŠ¸ ì½”ë“œ
    pass