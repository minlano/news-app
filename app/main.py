#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í‚¤ì›Œë“œ ë‰´ìŠ¤ ìš”ì•½ ë° ë¶„ì„ ì›¹ì•± ë©”ì¸ íŒŒì¼
Streamlit ê¸°ë°˜ ì›¹ ëŒ€ì‹œë³´ë“œ
"""

import streamlit as st
import pandas as pd
import json
import os
import sys
import atexit
import glob
from PIL import Image

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“ˆë“¤ì„ importí•˜ê¸° ìœ„í•œ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

try:
    from crawler.daum_crawler import DaumNewsCrawler
    from summarizer.text_summarizer import TextSummarizer
    from sentiment_analysis.sentiment import SentimentAnalyzer
    from report.report_generator import NewsReportGenerator
    from report.email_sender import EmailSender
    
    # keyword ëª¨ë“ˆ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ ì§ì ‘ import
    import importlib.util
    
    # keyword_extractor ì§ì ‘ ë¡œë“œ
    spec = importlib.util.spec_from_file_location("keyword_extractor", 
                                                  os.path.join(parent_dir, "keyword", "keyword_extractor.py"))
    keyword_extractor_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(keyword_extractor_module)
    KeywordExtractor = keyword_extractor_module.KeywordExtractor
    
    # wordcloud_gen ì§ì ‘ ë¡œë“œ
    spec = importlib.util.spec_from_file_location("wordcloud_gen", 
                                                  os.path.join(parent_dir, "keyword", "wordcloud_gen.py"))
    wordcloud_gen_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(wordcloud_gen_module)
    WordCloudGenerator = wordcloud_gen_module.WordCloudGenerator
    
except ImportError as e:
    st.error(f"ëª¨ë“ˆ import ì˜¤ë¥˜: {e}")
    st.stop()

def cleanup_temp_data():
    """ì„ì‹œ ë°ì´í„° íŒŒì¼ë“¤ ì‚­ì œ"""
    try:
        # ì‚­ì œí•  íŒŒì¼ íŒ¨í„´ë“¤
        temp_files = [
            'data/articles.json',
            'data/summarized_articles.json', 
            'data/keywords.json',
            'data/sentiment_analysis.json',
            'data/news_report_*.pdf',
            'data/sentiment_chart.png',
            'data/email_config.json',
            'data/wordcloud*.png'
        ]
        
        deleted_count = 0
        for pattern in temp_files:
            if '*' in pattern:
                # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ì²˜ë¦¬
                for file_path in glob.glob(pattern):
                    if os.path.exists(file_path):
                        os.remove(file_path)
                        deleted_count += 1
            else:
                # ì¼ë°˜ íŒŒì¼ ì²˜ë¦¬
                if os.path.exists(pattern):
                    os.remove(pattern)
                    deleted_count += 1
        
        if deleted_count > 0:
            print(f"ğŸ§¹ ì„ì‹œ ë°ì´í„° {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ì„ì‹œ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")

def clear_search_results():
    """ê²€ìƒ‰ ê²°ê³¼ ì´ˆê¸°í™”"""
    try:
        # ì‚­ì œí•  ê²€ìƒ‰ ê²°ê³¼ íŒŒì¼ë“¤
        search_files = [
            'data/articles.json',
            'data/summarized_articles.json', 
            'data/keywords.json',
            'data/sentiment_analysis.json',
            'data/wordcloud*.png'
        ]
        
        deleted_count = 0
        for pattern in search_files:
            if '*' in pattern:
                # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ì²˜ë¦¬
                for file_path in glob.glob(pattern):
                    if os.path.exists(file_path):
                        os.remove(file_path)
                        deleted_count += 1
            else:
                # ì¼ë°˜ íŒŒì¼ ì²˜ë¦¬
                if os.path.exists(pattern):
                    os.remove(pattern)
                    deleted_count += 1
        
        return deleted_count
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
        return 0

# í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì„ì‹œ ë°ì´í„° ìë™ ì‚­ì œ ë“±ë¡
atexit.register(cleanup_temp_data)

def load_data():
    """ì €ì¥ëœ ë°ì´í„° íŒŒì¼ë“¤ ë¡œë“œ"""
    data = {}
    
    # ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ
    try:
        with open('data/articles.json', 'r', encoding='utf-8') as f:
            data['articles'] = json.load(f)
    except FileNotFoundError:
        data['articles'] = []
    
    # ìš”ì•½ëœ ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ
    try:
        with open('data/summarized_articles.json', 'r', encoding='utf-8') as f:
            data['summarized_articles'] = json.load(f)
    except FileNotFoundError:
        data['summarized_articles'] = []
    
    # í‚¤ì›Œë“œ ë°ì´í„° ë¡œë“œ
    try:
        with open('data/keywords.json', 'r', encoding='utf-8') as f:
            data['keywords'] = json.load(f)
    except FileNotFoundError:
        data['keywords'] = {'keywords': [], 'total_keywords': 0}
    
    return data

def run_full_pipeline(keyword, max_articles=10):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # 1. í¬ë¡¤ë§
        if max_articles >= 15:
            status_text.text(f"ğŸ” ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘... ({max_articles}ê°œ ê¸°ì‚¬, 2-3ë¶„ ì†Œìš” ì˜ˆìƒ)")
        else:
            status_text.text(f"ğŸ” ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘... ({max_articles}ê°œ ê¸°ì‚¬)")
        progress_bar.progress(10)
        
        crawler = DaumNewsCrawler()
        articles = crawler.search_news(keyword, max_articles)
        
        if not articles:
            st.error("âŒ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        status_text.text(f"ğŸ“„ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ! ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
        progress_bar.progress(30)
        
        # ë³¸ë¬¸ ìˆ˜ì§‘
        for i, article in enumerate(articles):
            if i % 5 == 0:  # 5ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                status_text.text(f"ğŸ“– ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘... ({i+1}/{len(articles)})")
            content = crawler.get_article_content(article['link'])
            article['content'] = content
        
        crawler.save_articles(articles)
        
        # 2. ìš”ì•½
        status_text.text("ğŸ“ ê¸°ì‚¬ ìš”ì•½ ì¤‘...")
        progress_bar.progress(50)
        
        summarizer = TextSummarizer()
        summarized_articles = summarizer.summarize_articles(articles, method='simple')
        
        with open('data/summarized_articles.json', 'w', encoding='utf-8') as f:
            json.dump(summarized_articles, f, ensure_ascii=False, indent=2)
        
        # 3. í‚¤ì›Œë“œ ì¶”ì¶œ
        status_text.text("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„ ì¤‘...")
        progress_bar.progress(70)
        
        extractor = KeywordExtractor()
        keywords = extractor.extract_keywords_from_articles(articles, top_n=30)
        extractor.save_keywords(keywords)
        
        # 4. ê°ì„± ë¶„ì„
        status_text.text("ğŸ˜Š ê°ì„± ë¶„ì„ ì¤‘...")
        progress_bar.progress(75)
        
        analyzer = SentimentAnalyzer()
        analyzed_articles, sentiment_summary = analyzer.analyze_articles(articles)
        sentiment_stats = analyzer.get_sentiment_statistics(analyzed_articles)
        
        # ê°ì„± ë¶„ì„ ê²°ê³¼ ì €ì¥
        analyzer.save_sentiment_analysis(analyzed_articles)
        
        # 5. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        status_text.text("â˜ï¸ 3ê°€ì§€ ìŠ¤íƒ€ì¼ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘...")
        progress_bar.progress(90)
        
        generator = WordCloudGenerator()
        generator.create_multiple_styles()
        
        progress_bar.progress(100)
        status_text.text(f"âœ… ë¶„ì„ ì™„ë£Œ! {len(articles)}ê°œ ê¸°ì‚¬, {len(keywords)}ê°œ í‚¤ì›Œë“œ, ê°ì„±ë¶„ì„ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def display_articles(articles):
    """ê¸°ì‚¬ ëª©ë¡ í‘œì‹œ"""
    if not articles:
        st.info("ğŸ“­ í‘œì‹œí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    for i, article in enumerate(articles, 1):
        with st.expander(f"ğŸ“° {i}. {article['title']}", expanded=False):
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.write(f"**ì¶œì²˜:** {article.get('source', 'N/A')}")
                st.write(f"**ë§í¬:** {article.get('link', 'N/A')}")
                
                # AI ìš”ì•½ì´ ìˆìœ¼ë©´ í‘œì‹œ
                if article.get('ai_summary'):
                    st.write("**AI ìš”ì•½:**")
                    st.write(article['ai_summary'])
                else:
                    st.write("**ìš”ì•½:**")
                    st.write(article.get('summary', 'N/A')[:200] + "...")
            
            with col2:
                link_url = article.get('link', '#')
                if link_url and link_url != '#':
                    st.markdown(f"[ğŸ”— ì›ë¬¸ ë³´ê¸°]({link_url})")
                else:
                    st.write("ë§í¬ ì—†ìŒ")

def display_keywords(keywords_data):
    """í‚¤ì›Œë“œ í‘œì‹œ"""
    if not keywords_data or not keywords_data.get('keywords'):
        st.info("ğŸ“­ í‘œì‹œí•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    keywords = keywords_data['keywords'][:15]  # ìƒìœ„ 15ê°œ í‘œì‹œ
    
    # ë‘ ê°œ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ğŸ“Š í‚¤ì›Œë“œ ìˆœìœ„")
        # í‚¤ì›Œë“œ í…Œì´ë¸”
        df = pd.DataFrame(keywords)
        df.index = df.index + 1
        df.columns = ['í‚¤ì›Œë“œ', 'ë¹ˆë„']
        st.dataframe(df, use_container_width=True)
        
        # í‚¤ì›Œë“œ í´ë¦­ ê²€ìƒ‰ ê¸°ëŠ¥
        st.markdown("### ğŸ” í‚¤ì›Œë“œë¡œ ìƒˆë¡œ ê²€ìƒ‰")
        st.markdown("ì•„ë˜ í‚¤ì›Œë“œë¥¼ í´ë¦­í•˜ë©´ í•´ë‹¹ í‚¤ì›Œë“œë¡œ ìƒˆë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤:")
        
        # í‚¤ì›Œë“œ ë²„íŠ¼ë“¤ì„ 3ê°œì”© ë‚˜ëˆ„ì–´ í‘œì‹œ
        for i in range(0, min(12, len(keywords)), 3):
            button_cols = st.columns(3)
            for j, col in enumerate(button_cols):
                if i + j < len(keywords):
                    keyword_item = keywords[i + j]
                    keyword_word = keyword_item['word']
                    keyword_count = keyword_item['count']
                    
                    with col:
                        if st.button(f"ğŸ” {keyword_word} ({keyword_count})", key=f"search_{keyword_word}_{i}_{j}"):
                            # ì„¸ì…˜ ìƒíƒœì— ê²€ìƒ‰í•  í‚¤ì›Œë“œ ì €ì¥
                            st.session_state.search_keyword = keyword_word
                            st.experimental_rerun()
    
    with col2:
        st.subheader("ğŸ“ˆ ë¹ˆë„ ì°¨íŠ¸")
        # í‚¤ì›Œë“œ ë¹ˆë„ ì°¨íŠ¸
        chart_data = pd.DataFrame(keywords).set_index('word')['count']
        st.bar_chart(chart_data)

def main():
    """ë©”ì¸ ì›¹ì•± í•¨ìˆ˜"""
    
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="ğŸ“° í‚¤ì›Œë“œ ë‰´ìŠ¤ ìš”ì•½ ë¶„ì„",
        page_icon="ğŸ“°",
        layout="wide"
    )
    
    # ì œëª©
    st.title("ğŸ“° í‚¤ì›Œë“œ ë‰´ìŠ¤ ìš”ì•½ ë° ë¶„ì„ ì›¹ì•±")
    st.markdown("---")
    
    # ì„¸ì…˜ ìƒíƒœì—ì„œ ê²€ìƒ‰í•  í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
    if 'search_keyword' in st.session_state:
        keyword_from_session = st.session_state.search_keyword
        del st.session_state.search_keyword  # ì‚¬ìš© í›„ ì‚­ì œ
    else:
        keyword_from_session = ""
    
    # ì‚¬ì´ë“œë°”
    st.sidebar.header("ğŸ” ê²€ìƒ‰ ì„¤ì •")
    keyword = st.sidebar.text_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”", 
                                   value=keyword_from_session, 
                                   placeholder="ì˜ˆ: ë¶€ë™ì‚°, ì£¼ì‹, ì •ì¹˜")
    max_articles = st.sidebar.slider("ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜", 5, 20, 10)
    
    # ì‹œê°„ ê²½ê³ ë¬¸
    if max_articles >= 15:
        st.sidebar.warning("â° 15ê°œ ì´ìƒ ìˆ˜ì§‘ ì‹œ 2-3ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    elif max_articles >= 10:
        st.sidebar.info("â±ï¸ ì•½ 1-2ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤.")
    
    # ê²€ìƒ‰ ë²„íŠ¼ ë˜ëŠ” í‚¤ì›Œë“œ í´ë¦­ìœ¼ë¡œ ì¸í•œ ìë™ ê²€ìƒ‰
    search_triggered = st.sidebar.button("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„") or keyword_from_session
    
    if search_triggered and keyword.strip():
        with st.spinner("ë¶„ì„ ì¤‘..."):
            success = run_full_pipeline(keyword.strip(), max_articles)
            if success:
                st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
                st.experimental_rerun()
    elif search_triggered and not keyword.strip():
        st.sidebar.error("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    
    # ë°ì´í„° ë¡œë“œ
    data = load_data()
    
    # ë©”ì¸ ì»¨í…ì¸ 
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ï¿½ ë‰´ìŠ¤  ê¸°ì‚¬", "ğŸ” í‚¤ì›Œë“œ ë¶„ì„", "â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ", "ğŸ˜Š ê°ì„± ë¶„ì„", "ğŸ“Š PDF ë¦¬í¬íŠ¸"])
    
    with tab1:
        st.header("ğŸ“„ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡")
        
        # ìš”ì•½ëœ ê¸°ì‚¬ê°€ ìˆìœ¼ë©´ ìš°ì„  í‘œì‹œ, ì—†ìœ¼ë©´ ì›ë³¸ ê¸°ì‚¬ í‘œì‹œ
        articles_to_show = data['summarized_articles'] if data['summarized_articles'] else data['articles']
        
        if articles_to_show:
            st.info(f"ğŸ“Š ì´ {len(articles_to_show)}ê°œ ê¸°ì‚¬")
            display_articles(articles_to_show)
        else:
            # ì‚¬ìš©ë°©ë²• ì•ˆë‚´
            st.markdown("## ğŸš€ ì‚¬ìš©ë°©ë²•")
            
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.markdown("""
                ### ğŸ“ **1ë‹¨ê³„: í‚¤ì›Œë“œ ì…ë ¥**
                - ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ê´€ì‹¬ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”
                - ì˜ˆì‹œ: `ë¶€ë™ì‚°`, `ì£¼ì‹`, `ì •ì¹˜`, `ê²½ì œ`, `IT` ë“±
                
                ### ğŸ” **2ë‹¨ê³„: ê²€ìƒ‰ ì‹¤í–‰**
                - ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš” (5~20ê°œ)
                - "ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„" ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
                """)
            
            with col2:
                st.markdown("""
                ### ğŸ“Š **3ë‹¨ê³„: ê²°ê³¼ í™•ì¸**
                - **ë‰´ìŠ¤ ê¸°ì‚¬**: í¬ë¡¤ë§ëœ ê¸°ì‚¬ì™€ AI ìš”ì•½
                - **í‚¤ì›Œë“œ ë¶„ì„**: ì£¼ìš” í‚¤ì›Œë“œì™€ ë¹ˆë„ ì°¨íŠ¸
                - **ì›Œë“œí´ë¼ìš°ë“œ**: 3ê°€ì§€ ìŠ¤íƒ€ì¼ì˜ ì‹œê°í™”
                - **ê°ì„± ë¶„ì„**: ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ê°ì„± ë¶„ë¥˜
                - **PDF ë¦¬í¬íŠ¸**: ì „ì²´ ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸
                
                ### ğŸ’¡ **ì¶”ê°€ ê¸°ëŠ¥**
                - ì›Œë“œí´ë¼ìš°ë“œ ë‹¤ìš´ë¡œë“œ
                - PDF ë¦¬í¬íŠ¸ ì´ë©”ì¼ ë°œì†¡
                - í‚¤ì›Œë“œ í´ë¦­ìœ¼ë¡œ ì¬ê²€ìƒ‰
                """)
            
            st.markdown("---")
            st.markdown("### ğŸ¯ **ì£¼ìš” ê¸°ëŠ¥**")
            
            feature_col1, feature_col2, feature_col3, feature_col4 = st.columns(4)
            
            with feature_col1:
                st.markdown("""
                **ğŸ” ë‰´ìŠ¤ í¬ë¡¤ë§**
                - ë‹¤ìŒ ë‰´ìŠ¤ì—ì„œ ì‹¤ì‹œê°„ ìˆ˜ì§‘
                - ì œëª©, ë³¸ë¬¸, ìš”ì•½ ìë™ ì¶”ì¶œ
                - ìµœëŒ€ 20ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ê°€ëŠ¥
                """)
            
            with feature_col2:
                st.markdown("""
                **ğŸ“ AI ìš”ì•½**
                - ê¸´ ê¸°ì‚¬ë¥¼ ê°„ë‹¨í•˜ê²Œ ìš”ì•½
                - í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œ
                - ë¹ ë¥¸ ì •ë³´ íŒŒì•… ê°€ëŠ¥
                """)
            
            with feature_col3:
                st.markdown("""
                **ğŸ˜Š ê°ì„± ë¶„ì„**
                - ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ìë™ ë¶„ë¥˜
                - í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ì ìˆ˜
                - ì „ì²´ ê°ì„± ë™í–¥ íŒŒì•…
                """)
            
            with feature_col4:
                st.markdown("""
                **ğŸ“Š PDF ë¦¬í¬íŠ¸**
                - ì „ì²´ ë¶„ì„ ê²°ê³¼ í†µí•©
                - ì´ë©”ì¼ ìë™ ë°œì†¡
                - ì›Œë“œí´ë¼ìš°ë“œ í¬í•¨
                """)
            
            st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  ì‹œì‘í•´ë³´ì„¸ìš”!")
    
    with tab2:
        st.header("ğŸ” í‚¤ì›Œë“œ ë¶„ì„")
        
        if data['keywords']['keywords']:
            st.info(f"ğŸ“Š ì´ {data['keywords']['total_keywords']}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
            display_keywords(data['keywords'])
        else:
            st.info("ğŸ“­ ì•„ì§ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab3:
        st.header("â˜ï¸ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìŠ¤íƒ€ì¼ ì„ íƒ (3ê°€ì§€ë§Œ)
        available_styles = []
        style_files = {
            'default': 'data/wordcloud_default.png',
            'dark': 'data/wordcloud_dark.png', 
            'rainbow': 'data/wordcloud_rainbow.png'
        }
        
        # ì¡´ì¬í•˜ëŠ” íŒŒì¼ë“¤ë§Œ ì„ íƒì§€ì— ì¶”ê°€
        for style, filepath in style_files.items():
            if os.path.exists(filepath):
                available_styles.append(style)
        
        if available_styles:
            selected_style = st.selectbox(
                "ğŸ¨ ì›Œë“œí´ë¼ìš°ë“œ ìŠ¤íƒ€ì¼ ì„ íƒ:",
                available_styles,
                index=0
            )
            
            # ì„ íƒëœ ìŠ¤íƒ€ì¼ì˜ ì›Œë“œí´ë¼ìš°ë“œ í‘œì‹œ
            selected_path = style_files[selected_style]
            
            try:
                image = Image.open(selected_path)
                st.image(image, caption=f"{selected_style.title()} ìŠ¤íƒ€ì¼ ì›Œë“œí´ë¼ìš°ë“œ", use_column_width=True)
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                with open(selected_path, "rb") as file:
                    st.download_button(
                        label=f"ğŸ“¥ {selected_style} ìŠ¤íƒ€ì¼ ë‹¤ìš´ë¡œë“œ",
                        data=file.read(),
                        file_name=f"wordcloud_{selected_style}.png",
                        mime="image/png"
                    )
                    
            except Exception as e:
                st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
        else:
            st.info("ğŸ“­ ì•„ì§ ìƒì„±ëœ ì›Œë“œí´ë¼ìš°ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab4:
        st.header("ğŸ˜Š ê°ì„± ë¶„ì„")
        
        # ê°ì„± ë¶„ì„ ë°ì´í„° ë¡œë“œ
        try:
            with open('data/sentiment_analysis.json', 'r', encoding='utf-8') as f:
                sentiment_data = json.load(f)
            
            if sentiment_data and 'statistics' in sentiment_data:
                stats = sentiment_data['statistics']
                
                # ì „ì²´ ê°ì„± ìš”ì•½
                st.subheader("ğŸ“Š ì „ì²´ ê°ì„± ë¶„ì„ ê²°ê³¼")
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("ğŸ˜Š ê¸ì •ì  ê¸°ì‚¬", f"{stats['positive_count']}ê°œ", f"{stats['positive_ratio']}%")
                
                with col2:
                    st.metric("ğŸ˜ ë¶€ì •ì  ê¸°ì‚¬", f"{stats['negative_count']}ê°œ", f"{stats['negative_ratio']}%")
                
                with col3:
                    st.metric("ğŸ˜ ì¤‘ë¦½ì  ê¸°ì‚¬", f"{stats['neutral_count']}ê°œ", f"{stats['neutral_ratio']}%")
                
                with col4:
                    overall_sentiment = stats['overall_sentiment']
                    sentiment_emoji = "ğŸ˜Š" if overall_sentiment == "positive" else "ğŸ˜" if overall_sentiment == "negative" else "ğŸ˜"
                    st.metric("ğŸ¯ ì „ì²´ ê°ì„±", f"{sentiment_emoji} {overall_sentiment.upper()}", f"ì ìˆ˜: {stats['average_score']}")
                
                # ê°ì„± ë¶„í¬ ì°¨íŠ¸
                st.subheader("ğŸ“ˆ ê°ì„± ë¶„í¬")
                
                chart_data = pd.DataFrame({
                    'ê°ì„±': ['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½'],
                    'ê¸°ì‚¬ ìˆ˜': [stats['positive_count'], stats['negative_count'], stats['neutral_count']]
                })
                
                st.bar_chart(chart_data.set_index('ê°ì„±'))
                
                # ê°œë³„ ê¸°ì‚¬ ê°ì„± ë¶„ì„ ê²°ê³¼
                st.subheader("ğŸ“° ê°œë³„ ê¸°ì‚¬ ê°ì„± ë¶„ì„")
                
                if 'articles' in sentiment_data:
                    articles = sentiment_data['articles']
                    
                    for i, article in enumerate(articles[:10], 1):  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                        sentiment = article['sentiment']
                        sentiment_type = sentiment['sentiment']
                        sentiment_score = sentiment['score']
                        
                        # ê°ì„±ì— ë”°ë¥¸ ìƒ‰ìƒ ë° ì´ëª¨ì§€
                        if sentiment_type == 'positive':
                            color = "ğŸŸ¢"
                            emoji = "ğŸ˜Š"
                        elif sentiment_type == 'negative':
                            color = "ğŸ”´"
                            emoji = "ğŸ˜"
                        else:
                            color = "ğŸŸ¡"
                            emoji = "ğŸ˜"
                        
                        with st.expander(f"{color} {i}. {article['title'][:60]}... ({emoji} {sentiment_type.upper()})"):
                            col1, col2 = st.columns([3, 1])
                            
                            with col1:
                                st.write(f"**ê°ì„±:** {emoji} {sentiment_type.upper()}")
                                st.write(f"**ê°ì„± ì ìˆ˜:** {sentiment_score}")
                                st.write(f"**ê¸ì • í‚¤ì›Œë“œ:** {sentiment['positive_count']}ê°œ")
                                st.write(f"**ë¶€ì • í‚¤ì›Œë“œ:** {sentiment['negative_count']}ê°œ")
                                
                                if article.get('ai_summary'):
                                    st.write("**ìš”ì•½:**")
                                    st.write(article['ai_summary'])
                            
                            with col2:
                                link_url = article.get('link', '#')
                                if link_url and link_url != '#':
                                    st.markdown(f"[ğŸ”— ì›ë¬¸ ë³´ê¸°]({link_url})")
            else:
                st.info("ğŸ“­ ì•„ì§ ê°ì„± ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except FileNotFoundError:
            st.info("ğŸ“­ ì•„ì§ ê°ì„± ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ê°ì„± ë¶„ì„ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    with tab5:
        st.header("ğŸ“Š PDF ë¦¬í¬íŠ¸ ìƒì„±")
        
        # ë¦¬í¬íŠ¸ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        has_data = (os.path.exists('data/articles.json') and 
                   os.path.exists('data/keywords.json'))
        
        if has_data:
            st.subheader("ğŸ“„ ë¦¬í¬íŠ¸ ìƒì„±")
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown("""
                **í¬í•¨ë  ë‚´ìš©:**
                - ğŸ“Š ë¶„ì„ ê°œìš” ë° í†µê³„
                - ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡
                - ğŸ” ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„
                - ğŸ˜Š ê°ì„± ë¶„ì„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
                - â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€
                """)
            
            with col2:
                if st.button("ğŸ“„ PDF ë¦¬í¬íŠ¸ ìƒì„±", type="primary"):
                    with st.spinner("PDF ë¦¬í¬íŠ¸ ìƒì„± ì¤‘..."):
                        try:
                            # ë°ì´í„° ë¡œë“œ
                            with open('data/articles.json', 'r', encoding='utf-8') as f:
                                articles = json.load(f)
                            
                            with open('data/keywords.json', 'r', encoding='utf-8') as f:
                                keywords_data = json.load(f)
                            
                            # ê°ì„± ë¶„ì„ ë°ì´í„° ë¡œë“œ (ìˆëŠ” ê²½ìš°)
                            sentiment_stats = None
                            try:
                                with open('data/sentiment_analysis.json', 'r', encoding='utf-8') as f:
                                    sentiment_data = json.load(f)
                                    sentiment_stats = sentiment_data.get('statistics')
                            except FileNotFoundError:
                                pass
                            
                            # PDF ë¦¬í¬íŠ¸ ìƒì„±
                            generator = NewsReportGenerator()
                            pdf_path = generator.generate_report(
                                keyword=keyword if keyword else "ë¶„ì„ê²°ê³¼",
                                articles=articles,
                                keywords_data=keywords_data,
                                sentiment_stats=sentiment_stats
                            )
                            
                            if pdf_path:
                                st.success("âœ… PDF ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                
                                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                                with open(pdf_path, "rb") as pdf_file:
                                    st.download_button(
                                        label="ğŸ“¥ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ",
                                        data=pdf_file.read(),
                                        file_name=os.path.basename(pdf_path),
                                        mime="application/pdf"
                                    )
                            else:
                                st.error("âŒ PDF ë¦¬í¬íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                                
                        except Exception as e:
                            st.error(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ì´ë©”ì¼ ë°œì†¡ ê¸°ëŠ¥
            st.subheader("ğŸ“§ ì´ë©”ì¼ ë°œì†¡")
            
            # .env íŒŒì¼ ê¸°ë°˜ ê°„í¸ ë°œì†¡
            email_sender = EmailSender()
            env_config = email_sender.get_env_email_config()
            
            if env_config['gmail_email'] and env_config['gmail_password']:
                st.info(f"ğŸ“§ ë°œì†¡ì ì´ë©”ì¼: {env_config['gmail_email']}")
                
                recipient_email = st.text_input(
                    "ìˆ˜ì‹ ì ì´ë©”ì¼",
                    placeholder="recipient@example.com"
                )
                
                if st.button("ğŸ“§ ê°„í¸ ì´ë©”ì¼ ë°œì†¡", type="primary"):
                    if recipient_email:
                        with st.spinner("ì´ë©”ì¼ ë°œì†¡ ì¤‘..."):
                            try:
                                # ìµœì‹  PDF íŒŒì¼ ì°¾ê¸°
                                pdf_files = glob.glob("data/news_report_*.pdf")
                                if pdf_files:
                                    latest_pdf = max(pdf_files, key=os.path.getctime)
                                    
                                    # ê°ì„± ë¶„ì„ í†µê³„ ë¡œë“œ
                                    sentiment_stats = None
                                    try:
                                        with open('data/sentiment_analysis.json', 'r', encoding='utf-8') as f:
                                            sentiment_data = json.load(f)
                                            sentiment_stats = sentiment_data.get('statistics')
                                    except FileNotFoundError:
                                        pass
                                    
                                    # .env ê¸°ë°˜ ì´ë©”ì¼ ë°œì†¡
                                    success, message = email_sender.send_report_email_with_env(
                                        recipient_email=recipient_email,
                                        keyword=keyword if keyword else "ë¶„ì„ê²°ê³¼",
                                        article_count=len(data.get('articles', [])),
                                        pdf_path=latest_pdf,
                                        sentiment_stats=sentiment_stats
                                    )
                                    
                                    if success:
                                        st.success(f"âœ… {message}")
                                    else:
                                        st.error(f"âŒ {message}")
                                else:
                                    st.error("âŒ ë°œì†¡í•  PDF ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
                                    
                            except Exception as e:
                                st.error(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
                    else:
                        st.error("âŒ ìˆ˜ì‹ ì ì´ë©”ì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                st.warning("âš ï¸ .env íŒŒì¼ì— GMAIL_EMAILê³¼ GMAIL_APP_PASSWORDë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            

        else:
            st.info("ğŸ“­ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ë ¤ë©´ ë¨¼ì € ë‰´ìŠ¤ ê²€ìƒ‰ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    # í‘¸í„°
    st.markdown("---")
    st.markdown("ğŸ‰ **í‚¤ì›Œë“œ ë‰´ìŠ¤ ìš”ì•½ ë° ë¶„ì„ ì›¹ì•±**")
    st.markdown("ğŸ“Š ì‹¤ì‹œê°„ í¬ë¡¤ë§ | ğŸ“ AI ìš”ì•½ | ğŸ” í‚¤ì›Œë“œ ë¶„ì„ | â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™”")
    
    # ê¸°íƒ€ ê¸°ëŠ¥ ë²„íŠ¼ë“¤
    st.sidebar.markdown("---")
    st.sidebar.markdown("**ğŸ”„ ê¸°íƒ€ ê¸°ëŠ¥**")
    
    # ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼
    if st.sidebar.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨", help="í™”ë©´ì˜ ë°ì´í„°ë¥¼ ìµœì‹  ìƒíƒœë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤"):
        st.experimental_rerun()
    
    # ê²€ìƒ‰ ê²°ê³¼ ì´ˆê¸°í™” ë²„íŠ¼
    if st.sidebar.button("ğŸ—‘ï¸ ê²€ìƒ‰ ê²°ê³¼ ì´ˆê¸°í™”", help="ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ì™€ ë¶„ì„ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤"):
        with st.spinner("ê²€ìƒ‰ ê²°ê³¼ ì´ˆê¸°í™” ì¤‘..."):
            deleted_count = clear_search_results()
            if deleted_count > 0:
                st.sidebar.success(f"âœ… {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ!")
                st.experimental_rerun()
            else:
                st.sidebar.info("ğŸ“­ ì‚­ì œí•  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()